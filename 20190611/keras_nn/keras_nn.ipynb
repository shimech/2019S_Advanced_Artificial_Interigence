{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"keras_nn.ipynb のコピー","version":"0.3.2","provenance":[{"file_id":"1F4dcrUYHIgvhZp3ppdTC-i7vt7XXn6ks","timestamp":1560224469748}],"collapsed_sections":[]},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"MAeSuRYHN0YF","colab_type":"text"},"source":["# 0 Preface\n","このノートブックではニューラルネットワークを用いて，以下の3つの基本的な問題に取り組みます．\n","\n","\n","*   二値分類\n","*   多クラス分類\n","*   回帰\n","\n","ノートブック中のコードは (https://github.com/fchollet/deep-learning-with-python-notebooks) のコードを一部改変して使用しました．\n"]},{"cell_type":"markdown","metadata":{"id":"J53zK6iOAvd0","colab_type":"text"},"source":["初期設定のnumpyのバージョンでは，後ほど行うデータセットのダウンロードがうまくいかないため，ここではnumpyのバージョンを1.16.2とします．\n","<br>\n","エラーが出ますが，このハンズオンの実行には影響ありません．\n","<br>\n","(参考) https://github.com/tensorflow/tensorflow/issues/28102\n","\n","なお，この設定は現在実行中のcolabのインスタンスのみに適用されます．\n","<br>\n","(colabのインスタンスは生成されてから12時間もしくはセッションが切れてから90分でリセットされます．)"]},{"cell_type":"code","metadata":{"id":"dEnEsYCNTXxd","colab_type":"code","colab":{}},"source":["!pip install numpy==1.16.2\n","# !pip install numpy==1.16.2 folium==0.2.1 imgaug==0.2.5"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lL2qk7ynFANq","colab_type":"text"},"source":["Warningに書かれているように，ランタイムを再起動する必要があります．\n","[RESTART RUNTIME] をクリックしてください．"]},{"cell_type":"code","metadata":{"id":"DDBV22sTN0X7","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","import keras"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ycFULvhp_lXu","colab_type":"code","colab":{}},"source":["print(tf.test.gpu_device_name())"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GXCGN7V8_rfF","colab_type":"text"},"source":["'/device:GPU:0'と表示されればGPUが使えています．"]},{"cell_type":"markdown","metadata":{"id":"03c81O-_N0YF","colab_type":"text"},"source":["# 1 二値分類の例：映画レビューの分類\n","\n","この章では，映画レビューのテキストの内容に基づいて，映画レビューを肯定的なレビューと否定的なレビューに分類します．"]},{"cell_type":"markdown","metadata":{"id":"pEznA3WtN0YG","colab_type":"text"},"source":["## 1.1 IMDb データセット\n","\n","IMDb データセットはIMDb (Internet Movie Database) から収集された，50,000件の映画のレビューで構成されています．各レビューには，「肯定的」もしくは「否定的」のラベルが付けられており，それぞれ半々ずつあります．また，このデータセットは訓練用の25,000件のレビューとテスト用の25,000件のレビューに分かれています．\n","\n","MNISTデータセットと同様に，IMDbデータセットも前処理された状態でKerasに含まれています．レビューの内容（単語のシーケンス）は整数のシーケンスに変換されており，各整数はそれぞれ辞書の特定の単語を指しています．"]},{"cell_type":"code","metadata":{"id":"AOAuaGt1N0YH","colab_type":"code","colab":{}},"source":["from keras.datasets import imdb\n","\n","(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JaSLOsfXN0YJ","colab_type":"text"},"source":["* `train_data` : 訓練データのレビューのリスト\n","* `test_data` : テストデータのレビューのリスト\n","* `train_labels` : 訓練データのラベルのリスト\n","* `test_labels` : テストデータのラベルのリスト"]},{"cell_type":"markdown","metadata":{"id":"pNWgf_y3CypR","colab_type":"text"},"source":["まずは，データセットの中身を確認してみましょう．"]},{"cell_type":"code","metadata":{"id":"54ghRTYfN0YK","colab_type":"code","colab":{}},"source":["train_data[0]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lhWcZY1LN0YN","colab_type":"text"},"source":["各レビューは単語のインデックス（単語をエンコードしたもの）からなるリストです．"]},{"cell_type":"code","metadata":{"id":"s4iuk9DLN0YN","colab_type":"code","colab":{}},"source":["train_labels[0]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XM1AsVFDN0YR","colab_type":"text"},"source":["ラベルは0, 1の二値です．0は「否定的」，1は「肯定的」を意味します．"]},{"cell_type":"markdown","metadata":{"id":"3eO7h3-uN0YR","colab_type":"text"},"source":["ここでは，訓練データにおいて出現頻度が高い10,000個の単語のみを用い，出現頻度が低い単語は捨てています．そのため，単語のインデックスは10,000未満となっています．"]},{"cell_type":"code","metadata":{"id":"483TcdXxN0YS","colab_type":"code","colab":{}},"source":["max([max(sequence) for sequence in train_data])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Wab-PUWuN0YV","colab_type":"text"},"source":["参考までに，レビューをデコードして内容を確認してみましょう．"]},{"cell_type":"code","metadata":{"id":"rZUWF2IXN0YW","colab_type":"code","colab":{}},"source":["# word_index is a dictionary mapping words to an integer index\n","word_index = imdb.get_word_index()\n","# We reverse it, mapping integer indices to words\n","reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n","# We decode the review; note that our indices were offset by 3\n","# because 0, 1 and 2 are reserved indices for \"padding\", \"start of sequence\", and \"unknown\".\n","decoded_review = ' '.join([reverse_word_index.get(i - 3, '?') for i in train_data[0]])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q1ethWAnN0YZ","colab_type":"code","colab":{}},"source":["decoded_review"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mqQYA19NN0Yc","colab_type":"text"},"source":["## 1.2 データの前処理\n","\n","整数のリストをそのままニューラルネットワークに入力することはできません．ここでは，整数のリストをone-hotエンコーディングによって，10,000次元の2値ベクトルに変換します．\n","例えば，[3,5]というリストだった場合，3番目と5番目の要素が1でそれ以外は全て0の10,000次元ベクトルに変換します．"]},{"cell_type":"code","metadata":{"id":"v0w5sneEN0Yd","colab_type":"code","colab":{}},"source":["import numpy as np\n","\n","def vectorize_sequences(sequences, dimension=10000):\n","    # Create an all-zero matrix of shape (len(sequences), dimension)\n","    results = np.zeros((len(sequences), dimension))\n","    for i, sequence in enumerate(sequences):\n","        results[i, sequence] = 1.  # set specific indices of results[i] to 1s\n","    return results\n","\n","# Our vectorized training data\n","x_train = vectorize_sequences(train_data)\n","# Our vectorized test data\n","x_test = vectorize_sequences(test_data)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hV6Q70nQN0Yg","colab_type":"text"},"source":["レビューは次のような2値のnumpy配列に変換されています．"]},{"cell_type":"code","metadata":{"id":"TN6Nm62CN0Yh","colab_type":"code","colab":{}},"source":["x_train[0]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RYUPjxI9N0Yk","colab_type":"text"},"source":["ラベルも同様にnumpy配列に変換します．"]},{"cell_type":"code","metadata":{"id":"J7eUpMElN0Yl","colab_type":"code","colab":{}},"source":["# Our vectorized labels\n","y_train = np.asarray(train_labels).astype('float32')\n","y_test = np.asarray(test_labels).astype('float32')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ogq0GifMN0Yn","colab_type":"text"},"source":["## 1.3 ネットワークの構築\n","\n","ここでは下の図のような，全結合(Dense)層3層からなるネットワークを構築します．\n","なお，使用する層の数，各層の出力ユニット数については，改善の余地があります．\n","\n","![3-layer network](https://s3.amazonaws.com/book.keras.io/img/ch3/3_layer_network.png)"]},{"cell_type":"markdown","metadata":{"id":"21HgeGB6N0Yo","colab_type":"text"},"source":["Denseのコンストラクタの引数はその層の出力ユニットの数に対応し，キーワード引数は活性化関数と入力テンソルの形状に対応します．\n","入力テンソルの形状は，バッチ次元を含まないことに注意してください．\n","\n","中間層の出力ユニット数は16とし，活性化関数としてReLUを用います．\n","最終層の出力ユニット数は1とし，活性化関数としてシグモイド関数を用います．\n","シグモイド関数の出力は 0~1のスカラー値であり，そのレビューが肯定的である確率として解釈できます．"]},{"cell_type":"code","metadata":{"id":"Zqp9LQbTN0Yp","colab_type":"code","colab":{}},"source":["from keras import models\n","from keras import layers\n","\n","model = models.Sequential()\n","model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n","model.add(layers.Dense(16, activation='relu'))\n","model.add(layers.Dense(1, activation='sigmoid'))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oN7gZfl-N0Yu","colab_type":"text"},"source":["最後にモデルのコンパイルを行います．\n","二値分類問題では，損失関数として`binary_crossentropy`を用いるのが一般的です．\n","また，オプティマイザとして`rmsprop`を用います．さらに，訓練時に正解率も監視します．"]},{"cell_type":"code","metadata":{"id":"VAO23Pl3N0Yv","colab_type":"code","colab":{}},"source":["model.compile(optimizer='rmsprop',\n","              loss='binary_crossentropy',\n","              metrics=['acc'])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DMAr6z-6N0Y0","colab_type":"text"},"source":["## 1.4 アプローチの検証\n","\n","モデルの訓練時にテストデータを用いて評価を行うことは適切ではありません．\n","<br>\n","ここでは，訓練時に，訓練データに含まれていない新しいデータにおける正解率を監視するため，元の訓練データセットから取り分けておいた10,000個のサンプルを使って検証データセットを作成します．"]},{"cell_type":"code","metadata":{"id":"tnBwEEKuN0Y1","colab_type":"code","colab":{}},"source":["x_val = x_train[:10000]\n","partial_x_train = x_train[10000:]\n","\n","y_val = y_train[:10000]\n","partial_y_train = y_train[10000:]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"c0Y363SrN0Y5","colab_type":"text"},"source":["ミニバッチサイズは512とし，20エポックの訓練を行います．同時に，検証データにおける損失値と正解率を監視します．なお，検証データは引数`validation_data`で指定します．"]},{"cell_type":"code","metadata":{"id":"SpFikec-N0Y6","colab_type":"code","colab":{}},"source":["history = model.fit(partial_x_train,\n","                    partial_y_train,\n","                    epochs=20,\n","                    batch_size=512,\n","                    validation_data=(x_val, y_val))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QeO2hTqeN0Y_","colab_type":"text"},"source":["`model.fit()`は`History`オブジェクトを返します．このオブジェクトには`history`というメンバーがあり，訓練中に観測された全てのデータがディクショナリとして保存されています．"]},{"cell_type":"code","metadata":{"id":"FfYGNhvmN0ZA","colab_type":"code","colab":{}},"source":["history_dict = history.history\n","history_dict.keys()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XoDMB9QtN0ZE","colab_type":"text"},"source":["このディクショナリには，各エポックでの訓練データ，検証データそれぞれにおける正解率，損失値が含まれています．\n","matplotlibを用いてプロットしてみましょう．"]},{"cell_type":"code","metadata":{"id":"r705Mx4CN0ZE","colab_type":"code","colab":{}},"source":["import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","acc = history.history['acc']\n","val_acc = history.history['val_acc']\n","loss = history.history['loss']\n","val_loss = history.history['val_loss']\n","\n","epochs = range(1, len(acc) + 1)\n","\n","# \"bo\" is for \"blue dot\"\n","plt.plot(epochs, loss, 'bo', label='Training loss')\n","# b is for \"solid blue line\"\n","plt.plot(epochs, val_loss, 'b', label='Validation loss')\n","plt.title('Training and validation loss')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.legend()\n","\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4xFc8WK-N0ZH","colab_type":"code","colab":{}},"source":["plt.clf()   # clear figure\n","acc_values = history_dict['acc']\n","val_acc_values = history_dict['val_acc']\n","\n","plt.plot(epochs, acc, 'bo', label='Training acc')\n","plt.plot(epochs, val_acc, 'b', label='Validation acc')\n","plt.title('Training and validation accuracy')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.legend()\n","\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5FIKT19TN0ZJ","colab_type":"text"},"source":["点は訓練データでの結果，折れ線は検証データでの結果を表しています．\n","各図から，訓練データでの損失値はエポックごとに小さくなり，正解率はエポックごとに向上していることが分かります．\n","しかし，検証データでの損失値と正解率はそうはなっていません．\n","\n","これは過学習に陥っているためです．結局のところ，このモデルは訓練データに特化した表現を学習しているだけであり，訓練データ以外のデータに対して汎化していません．\n","\n","こういった場合，訓練を早期に中止することで，過学習を抑制することができます．\n","新しいモデルを4エポックで訓練し，テストデータで評価してみましょう．"]},{"cell_type":"code","metadata":{"id":"CFl0jKCfN0ZK","colab_type":"code","colab":{}},"source":["model = models.Sequential()\n","model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n","model.add(layers.Dense(16, activation='relu'))\n","model.add(layers.Dense(1, activation='sigmoid'))\n","\n","model.compile(optimizer='rmsprop',\n","              loss='binary_crossentropy',\n","              metrics=['accuracy'])\n","\n","model.fit(x_train, y_train, epochs=4, batch_size=512)\n","results = model.evaluate(x_test, y_test)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TgEShrSbN0ZS","colab_type":"code","colab":{}},"source":["results"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JdMfx0zGN0ZW","colab_type":"text"},"source":["このかなり単純なアプローチでも，88%ほどの正解率が達成されています．最先端のアプローチでは，95%近い正解率を達成することが可能です．"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Ff7twWHKc9O9"},"source":["## 1.5 新しいデータでの予測\n","\n","訓練済みのモデルを用いてテストデータでのレビューの評価を予測してみましょう．`predict`メソッドを用いることでレビューが肯定的だと判断される確率を計算できます．"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"2Rhu-niac9O-","colab":{}},"source":["model.predict(x_test)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"0PITJz9Vc9PB"},"source":["このモデルでは，確実に分類できるサンプル (0.99以上や0.01以下) もあれば，そうではないサンプルもあるようです．"]},{"cell_type":"markdown","metadata":{"id":"Qn2qgc5QaHFX","colab_type":"text"},"source":["## 1.6 過学習の対策"]},{"cell_type":"markdown","metadata":{"id":"eMoNmeYvdIf-","colab_type":"text"},"source":["### 1.6.1 重みの正則化\n","ネットワークの重みに小さい値だけが設定されるようにすることで，ネットワークが複雑になりすぎないようになり過学習を防ぐことができる可能性があります．\n","\n","重みの値をそのように制限することを **重みの正則化** と言います．重みを正則化するためには，重みの値に対するコストをネットワークの損失関数に加えます．代表的なコストには次のようなものがあります．\n","- L1正則化\n","\n","  追加されるコストは重み係数の絶対値(重みのL1ノルム)に比例します．\n","- L2正則化\n","\n","  追加されるコストは重み係数の値の二乗(重みのL2ノルム)に比例します．\n","  \n","先ほどまで作成していたネットワークにL2正則化を追加してみましょう．"]},{"cell_type":"code","metadata":{"id":"RHtMYROSYV6z","colab_type":"code","colab":{}},"source":["from keras import regularizers\n","\n","x_val = x_train[:10000]\n","partial_x_train = x_train[10000:]\n","y_val = y_train[:10000]\n","partial_y_train = y_train[10000:]\n","\n","model = models.Sequential()\n","model.add(layers.Dense(16, kernel_regularizer=regularizers.l2(0.001), activation='relu', input_shape=(10000,)))\n","model.add(layers.Dense(16, kernel_regularizer=regularizers.l2(0.001), activation='relu'))\n","model.add(layers.Dense(1, activation='sigmoid'))\n","\n","model.compile(optimizer='rmsprop',\n","              loss='binary_crossentropy',\n","              metrics=['accuracy'])\n","\n","history = model.fit(partial_x_train,\n","                    partial_y_train,\n","                    epochs=20,\n","                    batch_size=512,\n","                    validation_data=(x_val, y_val))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"v9cOnDDhZOwo","colab_type":"code","colab":{}},"source":["import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","acc = history.history['acc']\n","val_acc = history.history['val_acc']\n","loss = history.history['loss']\n","val_loss = history.history['val_loss']\n","\n","epochs = range(1, len(acc) + 1)\n","\n","# \"bo\" is for \"blue dot\"\n","plt.plot(epochs, loss, 'bo', label='Training loss')\n","# b is for \"solid blue line\"\n","plt.plot(epochs, val_loss, 'b', label='Validation loss')\n","plt.title('Training and validation loss')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.legend()\n","\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AShtqy8GcLql","colab_type":"text"},"source":["L2正則化を加えていない時に比べて，訓練データでのlossが大きい代わりに検証データでのロスが小さくなっていることが確認できると思います．"]},{"cell_type":"markdown","metadata":{"id":"XoHyOrFQdVqs","colab_type":"text"},"source":["### 1.6.2 ドロップアウト\n","\n","**ドロップアウト**とはニューラルネットワークにおいて最もよく使われている正則化手法の一つです．ドロップアウトを層に適用すると，訓練中にその層の出力特徴量の一部がランダムにdropされ0になります．\n","例えば，ある層の出力が[0.2, 0.5, 0.8, 1.3, 0.3]だったとして，ドロップアウトをその層に適用すると出力は[0, 0.5, 0, 1.3, 0.3]のようになります．(この結果はあくまで一例であり，0になる要素はランダムに選ばれます．)\n","\n","**ドロップアウト率**とは0にする要素の割合であり，通常0.2から0.5に設定されます．テスト時には，ドロップアウトは適用されません．そのため，訓練時よりも多くのユニットが活性化されてしまいます．これに折り合いをつけるため，テスト時にはドロップアウト率と同じ割合でスケールダウンされます．例えば，ドロップアウト率0.2で学習したならテスト時の出力は0.8倍されます．しかし，kerasなどのフレームワークを用いる際はライブラリが勝手にスケールダウンしてくれるため意識する必要はありません．映画分類ネットワークにドロップアウトを二つ追加してみましょう．"]},{"cell_type":"code","metadata":{"id":"WMfyLVX8eC1l","colab_type":"code","colab":{}},"source":["x_val = x_train[:10000]\n","partial_x_train = x_train[10000:]\n","y_val = y_train[:10000]\n","partial_y_train = y_train[10000:]\n","\n","model = models.Sequential()\n","model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n","model.add(layers.Dropout(0.5))\n","model.add(layers.Dense(16, activation='relu'))\n","model.add(layers.Dropout(0.5))\n","model.add(layers.Dense(1, activation='sigmoid'))\n","\n","model.compile(optimizer='rmsprop',\n","              loss='binary_crossentropy',\n","              metrics=['accuracy'])\n","\n","history = model.fit(partial_x_train,\n","                    partial_y_train,\n","                    epochs=20,\n","                    batch_size=512,\n","                    validation_data=(x_val, y_val))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HGidl0aph0fl","colab_type":"code","colab":{}},"source":["import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","acc = history.history['acc']\n","val_acc = history.history['val_acc']\n","loss = history.history['loss']\n","val_loss = history.history['val_loss']\n","\n","epochs = range(1, len(acc) + 1)\n","\n","# \"bo\" is for \"blue dot\"\n","plt.plot(epochs, loss, 'bo', label='Training loss')\n","# b is for \"solid blue line\"\n","plt.plot(epochs, val_loss, 'b', label='Validation loss')\n","plt.title('Training and validation loss')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.legend()\n","\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"02Jwt1VPh6ve","colab_type":"text"},"source":["こちらも，ドロップアウトを適用していない時に比べて訓練データでのlossが大きく検証データでのlossが小さくなっていることが確認できると思います．"]},{"cell_type":"markdown","metadata":{"id":"_rNvaxZhN0Zd","colab_type":"text"},"source":["# 2 多クラス分類の例：ニュース配信の分類\n","\n","前章では2クラスの場合の分類問題を扱いましたが，ここではクラスが3つ以上ある場合を考えます．\n","この章では，Reutersのニュース配信を46種類のトピックに分類します．"]},{"cell_type":"markdown","metadata":{"id":"W6BIP_bxN0Ze","colab_type":"text"},"source":["## 2.1 Reuters データセット\n","\n","\n","Reuters データセットは，1986年にReutersによって配信された短いニュース記事とそれらのトピックを集めたものです．トピックは46種類あります．トピック間でのサンプル数に偏りがあるものの，各トピックのサンプルが少なくとも10個は含まれています．\n","IMDbデータセットやMNISTデータセットと同様に，Reutersデータセットも前処理された状態でKerasに含まれています．"]},{"cell_type":"code","metadata":{"id":"D5SE6Bg1N0Ze","colab_type":"code","colab":{}},"source":["from keras.datasets import reuters\n","\n","(train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=10000)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zbLcCQcwN0Zh","colab_type":"text"},"source":["先ほどのIMDbデータセットの例と同様に，訓練データにおいて出現頻度が高い10,000個の単語のみを用い，出現頻度が低い単語は捨てています．\n","訓練サンプルは8,982個，テストサンプルは2,246個になります．"]},{"cell_type":"code","metadata":{"id":"PJR06_32N0Zk","colab_type":"code","colab":{}},"source":["len(train_data)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"n47BU-4LN0Zn","colab_type":"code","colab":{}},"source":["len(test_data)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"z-ZXqNyYN0Zs","colab_type":"text"},"source":["IMDbデータセットのレビューと同様に，各サンプルは単語のインデックスのリストです．"]},{"cell_type":"code","metadata":{"id":"haZaVEZ1N0Zt","colab_type":"code","colab":{}},"source":["train_data[10]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zpXV-xl_N0Zx","colab_type":"text"},"source":["参考までに，レビューをデコードして内容を確認してみましょう．"]},{"cell_type":"code","metadata":{"id":"a47PCke5N0Zy","colab_type":"code","colab":{}},"source":["word_index = reuters.get_word_index()\n","reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n","# Note that our indices were offset by 3\n","# because 0, 1 and 2 are reserved indices for \"padding\", \"start of sequence\", and \"unknown\".\n","decoded_newswire = ' '.join([reverse_word_index.get(i - 3, '?') for i in train_data[0]])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"z-5rjVllN0Z1","colab_type":"code","colab":{}},"source":["decoded_newswire"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JedGq6GMN0Z4","colab_type":"text"},"source":["サンプルに関連づけられているラベルは，0〜45の整数 (トピックインデックス) です．"]},{"cell_type":"code","metadata":{"id":"b8u1rndIN0Z4","colab_type":"code","colab":{}},"source":["train_labels[10]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_J_EnC9dN0Z9","colab_type":"text"},"source":["## 2.2 データの準備\n","\n","データのベクトル化は，IMDbデータセットの場合と同様の方法で行います．"]},{"cell_type":"code","metadata":{"id":"drh1y8zgN0Z-","colab_type":"code","colab":{}},"source":["import numpy as np\n","\n","def vectorize_sequences(sequences, dimension=10000):\n","    results = np.zeros((len(sequences), dimension))\n","    for i, sequence in enumerate(sequences):\n","        results[i, sequence] = 1.\n","    return results\n","\n","# Our vectorized training data\n","x_train = vectorize_sequences(train_data)\n","# Our vectorized test data\n","x_test = vectorize_sequences(test_data)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GeANCEr_N0aB","colab_type":"text"},"source":["ラベルは，one-hotエンコーディングによりベクトルします．"]},{"cell_type":"code","metadata":{"id":"rOsGMgLFN0aB","colab_type":"code","colab":{}},"source":["def to_one_hot(labels, dimension=46):\n","    results = np.zeros((len(labels), dimension))\n","    for i, label in enumerate(labels):\n","        results[i, label] = 1.\n","    return results\n","\n","# Our vectorized training labels\n","one_hot_train_labels = to_one_hot(train_labels)\n","# Our vectorized test labels\n","one_hot_test_labels = to_one_hot(test_labels)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LGZmmJqaN0aJ","colab_type":"text"},"source":["なおこの処理はKerasに組み込まれている関数`to_categorical`でも実行可能です．"]},{"cell_type":"code","metadata":{"id":"vafM8cbwN0aJ","colab_type":"code","colab":{}},"source":["from keras.utils.np_utils import to_categorical\n","\n","one_hot_train_labels = to_categorical(train_labels)\n","one_hot_test_labels = to_categorical(test_labels)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SHWdwKsRN0aP","colab_type":"text"},"source":["## 2.3 ネットワークの構築\n","\n","多クラス分類の場合，ネットワークの出力が各クラスに属する確率に対応するため，最終層のユニット数はクラス数となります．\n","ここでは，46種類のクラスを学習するため，前章の例より出力空間の次元がはるかに大きくなります．\n","そのため，前章の例のように中間層を16次元とした場合，中間層が情報ボトルネックとなり，重要な情報が失われてしまう恐れがあります．\n","したがって，ここではもっと大きな64ユニットの中間層を用います．\n","\n","また，最終層の活性関数は`Softmax`とします．`i`番目の出力がクラス`i`に属している確率を表し，また確率値の合計が`1`となります．"]},{"cell_type":"code","metadata":{"id":"Z3tdi8ggN0aQ","colab_type":"code","colab":{}},"source":["from keras import models\n","from keras import layers\n","\n","model = models.Sequential()\n","model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n","model.add(layers.Dense(64, activation='relu'))\n","model.add(layers.Dense(46, activation='softmax'))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L5RHDIknN0aU","colab_type":"text"},"source":["多クラス分類では，損失関数として`categorical_crossentropy`を用いるのが一般的です．"]},{"cell_type":"code","metadata":{"id":"G0G4v8Q0N0aW","colab_type":"code","colab":{}},"source":["model.compile(optimizer='rmsprop',\n","              loss='categorical_crossentropy',\n","              metrics=['accuracy'])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pyQJJHtjN0aZ","colab_type":"text"},"source":["## 2.4 アプローチの検証\n","\n","訓練データのうち1,000サンプルを検証データセットとして使用するために分けておきます．"]},{"cell_type":"code","metadata":{"id":"REi6nJpzN0aa","colab_type":"code","colab":{}},"source":["x_val = x_train[:1000]\n","partial_x_train = x_train[1000:]\n","\n","y_val = one_hot_train_labels[:1000]\n","partial_y_train = one_hot_train_labels[1000:]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KlRwSloDN0ae","colab_type":"text"},"source":["次に，512サンプルのミニバッチで20エポックの訓練を行います．"]},{"cell_type":"code","metadata":{"id":"yWiydjh0N0af","colab_type":"code","colab":{}},"source":["history = model.fit(partial_x_train,\n","                    partial_y_train,\n","                    epochs=20,\n","                    batch_size=512,\n","                    validation_data=(x_val, y_val))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QeQdg4VBN0aj","colab_type":"text"},"source":["最後に，損失値と正解率をプロットします．"]},{"cell_type":"code","metadata":{"id":"c4v9IMLoN0aj","colab_type":"code","colab":{}},"source":["import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","loss = history.history['loss']\n","val_loss = history.history['val_loss']\n","\n","epochs = range(1, len(loss) + 1)\n","\n","plt.plot(epochs, loss, 'bo', label='Training loss')\n","plt.plot(epochs, val_loss, 'b', label='Validation loss')\n","plt.title('Training and validation loss')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.legend()\n","\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"As1Jjp7mN0an","colab_type":"code","colab":{}},"source":["plt.clf()   # clear figure\n","\n","acc = history.history['acc']\n","val_acc = history.history['val_acc']\n","\n","plt.plot(epochs, acc, 'bo', label='Training acc')\n","plt.plot(epochs, val_acc, 'b', label='Validation acc')\n","plt.title('Training and validation accuracy')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.legend()\n","\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3lTl6Zn4N0at","colab_type":"text"},"source":["8エポックの後，このネットワークは過学習に陥っています．新しいネットワークを8エポックで訓練し，その後テストデータで評価してみましょう．"]},{"cell_type":"code","metadata":{"id":"SN-bfJGtN0at","colab_type":"code","colab":{}},"source":["model = models.Sequential()\n","model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n","model.add(layers.Dense(64, activation='relu'))\n","model.add(layers.Dense(46, activation='softmax'))\n","\n","model.compile(optimizer='rmsprop',\n","              loss='categorical_crossentropy',\n","              metrics=['accuracy'])\n","model.fit(partial_x_train,\n","          partial_y_train,\n","          epochs=8,\n","          batch_size=512,\n","          validation_data=(x_val, y_val))\n","results = model.evaluate(x_test, one_hot_test_labels)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5GO17uW5N0aw","colab_type":"code","colab":{}},"source":["results"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iqUDitQFN0az","colab_type":"text"},"source":["このアプローチの正解率は78%程度となりました．"]},{"cell_type":"code","metadata":{"id":"CMoYf5F3N0az","colab_type":"code","colab":{}},"source":["import copy\n","\n","test_labels_copy = copy.copy(test_labels)\n","np.random.shuffle(test_labels_copy)\n","float(np.sum(np.array(test_labels) == np.array(test_labels_copy))) / len(test_labels)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2tREdrYZN0a3","colab_type":"text"},"source":["ランダムなベースラインの正解率は19%程度となっています．そのため78%という結果は，よい結果であるといえます．"]},{"cell_type":"markdown","metadata":{"id":"dmyUPP-GN0a5","colab_type":"text"},"source":["## 2.5 新しいデータでの予測\n","\n","このモデルの`predict`メソッドは，46種類ある各トピックの確率分布を返します．このことを検証するために，テストデータ全体のトピック予測を計算してみましょう．"]},{"cell_type":"code","metadata":{"id":"QUgFi72zN0a7","colab_type":"code","colab":{}},"source":["predictions = model.predict(x_test)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_bY7UXAyN0bB","colab_type":"text"},"source":["`predictions`の各エントリは，長さが46のベクトルです．"]},{"cell_type":"code","metadata":{"id":"sDckCQNcN0bC","colab_type":"code","colab":{}},"source":["predictions[0].shape"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fj1zt2eAN0bE","colab_type":"text"},"source":["このベクトルの和は1になります．"]},{"cell_type":"code","metadata":{"id":"5AhauWNeN0bF","colab_type":"code","colab":{}},"source":["np.sum(predictions[0])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KlY-h7vwN0bL","colab_type":"text"},"source":["最大となるエントリが，確率が最も高いクラス，つまり予測クラスです．"]},{"cell_type":"code","metadata":{"id":"NAAUqp5tN0bL","colab_type":"code","colab":{}},"source":["np.argmax(predictions[0])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CV2S5laEN0bO","colab_type":"text"},"source":["## 2.5 ラベルと損失値を処理する別の方法\n","\n","ラベルをone-hotエンコードするのではなく，整数のテンソルとしてキャストすることも可能です．"]},{"cell_type":"code","metadata":{"id":"5pw1E9QFN0bP","colab_type":"code","colab":{}},"source":["y_train = np.array(train_labels)\n","y_test = np.array(test_labels)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FdeSIlhnN0bR","colab_type":"text"},"source":["この場合，損失関数として，one-hotエンコーディングの際に用いた`categorical_crossentropy`ではなく，`sparse_categorical_crossentropy`を用います．"]},{"cell_type":"code","metadata":{"id":"88ZFXcGYN0bR","colab_type":"code","colab":{}},"source":["model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', metrics=['acc'])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_44dLLv4N0bT","colab_type":"text"},"source":["この損失関数は，数学的には`sparse_categorical_crossentropy`と同じで，違いはインターフェイスの部分だけです．"]},{"cell_type":"markdown","metadata":{"id":"Pnbu8D4sN0bT","colab_type":"text"},"source":["## 2.6 十分な大きさの中間層を持つことの重要性\n","\n","最終的な出力は46次元であるため，ユニットの数が46よりもずっと小さい中間層は避けるべきです．\n","たとえば4次元の中間層を用いた場合はどうなるでしょうか．実際に試してみましょう．"]},{"cell_type":"code","metadata":{"id":"mDHIfMnIN0bT","colab_type":"code","colab":{}},"source":["model = models.Sequential()\n","model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n","model.add(layers.Dense(4, activation='relu'))\n","model.add(layers.Dense(46, activation='softmax'))\n","\n","model.compile(optimizer='rmsprop',\n","              loss='categorical_crossentropy',\n","              metrics=['accuracy'])\n","model.fit(partial_x_train,\n","          partial_y_train,\n","          epochs=20,\n","          batch_size=128,\n","          validation_data=(x_val, y_val))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fWhSVCGaN0bV","colab_type":"text"},"source":["検証データセットでの正解率のピークは71%程度であり，先ほどよりも低い値となっています．これは，46クラスの分離超平面を復元するのに必要な情報を，あまりにも小さな中間層に詰め込もうとしたことが原因です．このネットワークには，これらの表現に必要な情報の大部分は詰め込むことができますが，必要な情報がすべて詰め込まれるわけではありません．"]},{"cell_type":"markdown","metadata":{"id":"B-6gC8HyN0bW","colab_type":"text"},"source":["# 3 回帰の例：住宅価格の予測\n","\n","これまでの章では，離散的なラベルを予測する分類問題を取り上げました．この章では，連続値を予測する回帰問題を扱います．"]},{"cell_type":"markdown","metadata":{"id":"aa3Cp5gBN0bW","colab_type":"text"},"source":["## 3.1 Boston Housing データセット\n","\n","ここでは，1970年代中頃のボストン近郊での住宅価格の中央値を予測します．この予測には，犯罪発生率や地方財産税の税率など，当時のボストン近郊に関するデータを使用します．\n","\n","このデータセットに含まれているデータは506個と比較的少なく，404の訓練サンプルと102のテストサンプルに分割されています．また，入力データの特徴量はそれぞれ異なる尺度を用いています．たとえば，割合を0〜1の値で表すものもあれば，1〜12の値をとるものや，0〜100の値をとるものもあります．"]},{"cell_type":"code","metadata":{"id":"PoObwkeZN0bX","colab_type":"code","colab":{}},"source":["from keras.datasets import boston_housing\n","\n","(train_data, train_targets), (test_data, test_targets) =  boston_housing.load_data()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"XG9DnzYWN0bZ","colab_type":"code","colab":{}},"source":["train_data.shape"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"WOifUK_jN0bb","colab_type":"code","colab":{}},"source":["test_data.shape"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IajMbRuxN0be","colab_type":"text"},"source":["各サンプルは13種類の数値の特徴量で構成されています．これらの特徴量は，犯罪発生率，1戸あたりの平均部屋数，幹線道路へのアクセス指数などを表します．"]},{"cell_type":"code","metadata":{"id":"KVr8ZEyeN0bf","colab_type":"code","colab":{}},"source":["train_targets"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"95my44O6N0bg","colab_type":"text"},"source":["目標値は，住宅価格の中央値 (1,000ドル単位) です．住宅価格は主に10,000ドルから50,000ドルの間です．"]},{"cell_type":"markdown","metadata":{"id":"Nfu9Foj1N0bh","colab_type":"text"},"source":["## 3.2 データの正規化\n","\n","異なる範囲の値をとる特徴量を用いて学習を行うことは困難です．そこで，各特徴量ごとに正規化を行います．\n","\n","具体的には各特徴量ごとに，平均値を引き，標準偏差で割ることによって各特徴量の平均を0，標準偏差を1に揃えます．"]},{"cell_type":"code","metadata":{"id":"paJriXXYN0bh","colab_type":"code","colab":{}},"source":["mean = train_data.mean(axis=0)\n","train_data -= mean\n","std = train_data.std(axis=0)\n","train_data /= std\n","\n","test_data -= mean\n","test_data /= std"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8Qjw5B1bN0bi","colab_type":"text"},"source":["## 3.3 ネットワークの構築\n","\n","訓練データが少ないため，2つの中間層をもつ小さなニューラルネットワークを用います．中間層のユニット数は64とします．一般的に，訓練データが少なければ少ないほど，モデルは過学習に陥りやすくなります．小さいネットワークを使用することで，過学習を抑制することができます．"]},{"cell_type":"code","metadata":{"id":"txC3G9txN0bj","colab_type":"code","colab":{}},"source":["from keras import models\n","from keras import layers\n","\n","def build_model():\n","    # Because we will need to instantiate\n","    # the same model multiple times,\n","    # we use a function to construct it.\n","    model = models.Sequential()\n","    model.add(layers.Dense(64, activation='relu',\n","                           input_shape=(train_data.shape[1],)))\n","    model.add(layers.Dense(64, activation='relu'))\n","    model.add(layers.Dense(1))\n","    model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])\n","    return model"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"emjq2hvjN0bk","colab_type":"text"},"source":["最終層のユニットは1つだけで，活性化関数を適用していません．この場合，最終層は完全に線形の層となり任意の値を予測することが可能です．\n","\n","また，損失関数として平均二乗誤差 (mean square error, MSE) を用いています．これは，予測値と目的値との差の自乗であり，回帰問題の損失関数として広く使用されています．\n","\n","訓練時には，平均絶対誤差 (mean absolute error, MAE) という新しい指標も監視しています．これは，予測値と目的値との差の絶対値です．例えば，MAEが0.5のとき，予測値は平均で500ドルずれていることになります．"]},{"cell_type":"markdown","metadata":{"id":"wQbbGTfON0bl","colab_type":"text"},"source":["## 3.4 k分割交差検証によるアプローチの検証\n","\n","これまでの章では，データを訓練データセットと検証データセットに分割することでネットワークの評価を行ってきました．\n","しかし今回の場合のように，データの数が少ない場合，検証データセットも小さくなり，検証データセットの分割方法によっては，検証スコアのバリアンスが高くなり，過学習に陥ります．これでは，モデルを正確に評価することは不可能です．\n","\n","こうした状況では，k分割交差検証が適しています．k分割交差検証では，利用可能なデータをK個のフォールドに分割し，まったく同じモデルのインスタンスをK個作成します．そして，各モデルをK-1個のフォールドで訓練し，残りの1個のフォールドで評価します．そして最後に，K個の検証スコアの平均を求めます．通常，Kの値は4か5とします．"]},{"cell_type":"code","metadata":{"id":"mZubny9dN0bl","colab_type":"code","colab":{}},"source":["import numpy as np\n","\n","k = 4\n","num_val_samples = len(train_data) // k\n","num_epochs = 100\n","all_scores = []\n","for i in range(k):\n","    print('processing fold #', i)\n","    # Prepare the validation data: data from partition # k\n","    val_data = train_data[i * num_val_samples: (i + 1) * num_val_samples]\n","    val_targets = train_targets[i * num_val_samples: (i + 1) * num_val_samples]\n","\n","    # Prepare the training data: data from all other partitions\n","    partial_train_data = np.concatenate(\n","        [train_data[:i * num_val_samples],\n","         train_data[(i + 1) * num_val_samples:]],\n","        axis=0)\n","    partial_train_targets = np.concatenate(\n","        [train_targets[:i * num_val_samples],\n","         train_targets[(i + 1) * num_val_samples:]],\n","        axis=0)\n","\n","    # Build the Keras model (already compiled)\n","    model = build_model()\n","    # Train the model (in silent mode, verbose=0)\n","    model.fit(partial_train_data, partial_train_targets,\n","              epochs=num_epochs, batch_size=16, verbose=0)\n","    # Evaluate the model on the validation data\n","    val_mse, val_mae = model.evaluate(val_data, val_targets, verbose=0)\n","    all_scores.append(val_mae)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dhqbO20bN0bo","colab_type":"code","colab":{}},"source":["all_scores"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2E1DhNp5N0bt","colab_type":"code","colab":{}},"source":["np.mean(all_scores)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ak5l6WTnN0bv","colab_type":"text"},"source":["フォールドによって検証スコアにばらつきがあることがわかります．k分割交差検証ではそれらの平均値を検証スコアとします．この場合，誤差の平均は2,400ドルです．住宅価格が10,000ドルから50,000ドルであることを考えると，これはかなり大きな誤差です．\n","\n","このネットワークをもう500エポック訓練してみましょう．"]},{"cell_type":"code","metadata":{"id":"38gpudOvN0bw","colab_type":"code","colab":{}},"source":["from keras import backend as K\n","\n","# Some memory clean-up\n","K.clear_session()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fuUJTYnRN0by","colab_type":"code","colab":{}},"source":["num_epochs = 500\n","all_mae_histories = []\n","for i in range(k):\n","    print('processing fold #', i)\n","    # Prepare the validation data: data from partition # k\n","    val_data = train_data[i * num_val_samples: (i + 1) * num_val_samples]\n","    val_targets = train_targets[i * num_val_samples: (i + 1) * num_val_samples]\n","\n","    # Prepare the training data: data from all other partitions\n","    partial_train_data = np.concatenate(\n","        [train_data[:i * num_val_samples],\n","         train_data[(i + 1) * num_val_samples:]],\n","        axis=0)\n","    partial_train_targets = np.concatenate(\n","        [train_targets[:i * num_val_samples],\n","         train_targets[(i + 1) * num_val_samples:]],\n","        axis=0)\n","\n","    # Build the Keras model (already compiled)\n","    model = build_model()\n","    # Train the model (in silent mode, verbose=0)\n","    history = model.fit(partial_train_data, partial_train_targets,\n","                        validation_data=(val_data, val_targets),\n","                        epochs=num_epochs, batch_size=16, verbose=0)\n","    mae_history = history.history['val_mean_absolute_error']\n","    all_mae_histories.append(mae_history)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DaLHFxQYN0b1","colab_type":"text"},"source":["続いて，すべてのフォールドを対象に，エポックごとのMAEスコアの平均を求めます．"]},{"cell_type":"code","metadata":{"id":"yFXZ6DdjN0b2","colab_type":"code","colab":{}},"source":["average_mae_history = [\n","    np.mean([x[i] for x in all_mae_histories]) for i in range(num_epochs)]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rpbZkASaN0b5","colab_type":"text"},"source":["結果をプロットしましょう．"]},{"cell_type":"code","metadata":{"id":"UHcvo7KoN0b7","colab_type":"code","colab":{}},"source":["import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","plt.plot(range(1, len(average_mae_history) + 1), average_mae_history)\n","plt.xlabel('Epochs')\n","plt.ylabel('Validation MAE')\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"g9gaqg-CN0cA","colab_type":"text"},"source":["スケーリングの問題やバリアンスが比較的高いせいで，このプロットを読んで理解するのは少々難しいですね．そこで，次のようにしてみましょう．\n","* 最初の10個のデータ点を省略する (尺度が異なるため)\n","* 各データ点をその手前にあるデータ点の指数移動平均に置き換えることで，なめらかな曲線にする．"]},{"cell_type":"code","metadata":{"id":"N-xa7xInN0cA","colab_type":"code","colab":{}},"source":["def smooth_curve(points, factor=0.9):\n","    smoothed_points = []\n","    for point in points:\n","        if smoothed_points:\n","            previous = smoothed_points[-1]\n","            smoothed_points.append(previous * factor + point * (1 - factor))\n","        else:\n","            smoothed_points.append(point)\n","    return smoothed_points\n","\n","smooth_mae_history = smooth_curve(average_mae_history[10:])\n","\n","plt.plot(range(1, len(smooth_mae_history) + 1), smooth_mae_history)\n","plt.xlabel('Epochs')\n","plt.ylabel('Validation MAE')\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZOXn6LXQN0cD","colab_type":"text"},"source":["このプロットから，検証スコア (MAE) が80エポック程度のときに最適になっていることがわかります．その後は過学習に陥っています．\n","\n","モデルの他のパラメータのチューニングが完了したら，最適なパラメータと訓練データ全体を使って最終的なモデルの訓練をします．続いて，テストデータでの性能を調べます．"]},{"cell_type":"code","metadata":{"id":"rfLINPtVN0cD","colab_type":"code","colab":{}},"source":["# Get a fresh, compiled model.\n","model = build_model()\n","# Train it on the entirety of the data.\n","model.fit(train_data, train_targets,\n","          epochs=80, batch_size=16, verbose=0)\n","test_mse_score, test_mae_score = model.evaluate(test_data, test_targets)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"msDBuQJxN0cF","colab_type":"code","colab":{}},"source":["test_mae_score"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GMiOJiHmN0cH","colab_type":"text"},"source":["依然として2,500ドル程度もずれています．"]}]}