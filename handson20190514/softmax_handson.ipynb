{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"softmax_handson.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"dTC4iCLP0sVG","colab_type":"text"},"source":["# ソフトマックス回帰　(softmax regression)の実装\n","# 目次\n","1. 概要\n","- 目標\n","- 下準備\n","- softmax関数の実装\n","- 多クラス交差エントロピー誤差の実装\n","- ソフトマックス回帰クラスの実装\n","- 学習\n","\n","# 1. 概要\n","- ロジスティック回帰の一般化で，多クラスに対応した手法\n","- K個のクラス識別問題を考える．$\\boldsymbol{x}$：入力データ，$\\boldsymbol{t}$：教師データ\n","\n","\\begin{align}\n","\\it{D}=\\left\\{\\left(\\boldsymbol{x}_i,\\boldsymbol{t}_i\\right)\\right\\}_{i=1}^{N}\\, ,\\boldsymbol{x}\\in\\mathbb{R}^d,\\, \\boldsymbol{t}\\in\\left\\{1,\\cdots,K\\right\\}\n","\\end{align}\n","\n","- 各クラスの事後確率を求める．各クラスごとに重み行列 $\\boldsymbol{w}^{(𝑗)}$を持つ．\n","$$\n","P(y=1|\\,\\boldsymbol{x})=\\frac{\\exp({\\boldsymbol{w}^{(1)\\top}\\boldsymbol{x}})}{\\sum_{j=1}^{K}\\exp{(\\boldsymbol{w}^{(j)\\top}\\boldsymbol{x}})}\\\\\n","P(y=2|\\,\\boldsymbol{x})=\\frac{\\exp({\\boldsymbol{w}^{(2)\\top}\\boldsymbol{x}})}{\\sum_{j=1}^{K}\\exp{(\\boldsymbol{w}^{(j)\\top}\\boldsymbol{x}})}\\\\\n","\\vdots\\\\\n","P(y=K|\\,\\boldsymbol{x})=\\frac{\\exp({\\boldsymbol{w}^{(K)\\top}\\boldsymbol{x}})}{\\sum_{j=1}^{K}\\exp{(\\boldsymbol{w}^{(j)\\top}\\boldsymbol{x}})}\n","$$"]},{"cell_type":"markdown","metadata":{"id":"76H1sytv0sVI","colab_type":"text"},"source":["# 2. 目標\n","- ソフトマックス回帰を実装して，mnist（手書き数字データセット）を識別する．\n","- まず**活性化関数**の一種である**softmax**関数と，**交差エントロピー関数**を実装する．その後確率**的勾配降下法**を実装し，**SoftmaxRegression**クラスを実装する．"]},{"cell_type":"markdown","metadata":{"id":"AwatT0oB0sVJ","colab_type":"text"},"source":["# 3. 下準備\n","## 3.1 ライブラリのインポート\n","- matplotlib: 図やグラフの描画など．\n","- numpy: 行列演算など\n","- sklearn: scikit-learn．様々な機械学習のモデルが利用できるが，今回はMNISTのデータをダウンロードするのに用いる．"]},{"cell_type":"code","metadata":{"slideshow":{"slide_type":"slide"},"id":"PRi2I9olK7T1","colab_type":"code","colab":{}},"source":["%matplotlib inline\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from sklearn.datasets import fetch_mldata\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","from __future__ import print_function"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yBn555zkVXAO","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"84619125-0e77-44b8-dc08-5ffd00cca52f","executionInfo":{"status":"ok","timestamp":1557816622513,"user_tz":-540,"elapsed":890,"user":{"displayName":"Shuntaro Shimizu","photoUrl":"","userId":"02252545979345483385"}}},"source":["from google.colab import drive # driveを接続\n","drive.mount('/content/gdrive')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"7gHPHdZLVfT4","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"41f82de6-a1df-4eef-f38f-10e5ef5e0de9","executionInfo":{"status":"ok","timestamp":1557816641003,"user_tz":-540,"elapsed":595,"user":{"displayName":"Shuntaro Shimizu","photoUrl":"","userId":"02252545979345483385"}}},"source":["# drive中の課題ファイルのあるディレクトリに移動\n","%cd /content/gdrive/My Drive/先端人工知能論Ⅰ/handson20190514\n","\n","from test_softmax import *"],"execution_count":3,"outputs":[{"output_type":"stream","text":["/content/gdrive/My Drive/先端人工知能論Ⅰ/handson20190514\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"RPI09MhY0sVQ","colab_type":"text"},"source":["## 3.2 MNISTデータの読み込み\n","- データをダウンロードする．一度ダウンロードすると，その後はデータを参照して読み込んでくれるので，毎回ダウンロードしなくても良くなる．\n","- Xが画像データ，Yが正解データ\n","- mnistのデータは，0~255のint型で表されているが，これを**255で割って**正規化する．"]},{"cell_type":"code","metadata":{"id":"_VFl1sNe0sVR","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":105},"outputId":"7c38c1b1-352c-48ef-f331-00c7d40c1078","executionInfo":{"status":"ok","timestamp":1557816642984,"user_tz":-540,"elapsed":1122,"user":{"displayName":"Shuntaro Shimizu","photoUrl":"","userId":"02252545979345483385"}}},"source":["mnist = fetch_mldata('MNIST original', data_home='./data/')\n","X, Y = mnist.data, mnist.target\n","X = X / 255.\n","Y = Y.astype(\"int\")"],"execution_count":4,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:77: DeprecationWarning: Function fetch_mldata is deprecated; fetch_mldata was deprecated in version 0.20 and will be removed in version 0.22\n","  warnings.warn(msg, category=DeprecationWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:77: DeprecationWarning: Function mldata_filename is deprecated; mldata_filename was deprecated in version 0.20 and will be removed in version 0.22\n","  warnings.warn(msg, category=DeprecationWarning)\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"U4WlHWs10sVT","colab_type":"text"},"source":["### データセットの可視化"]},{"cell_type":"code","metadata":{"id":"SFtmq1PO0sVU","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":227},"outputId":"6f1c2ec0-d6ce-4333-d946-27727619d02e","executionInfo":{"status":"ok","timestamp":1557816644552,"user_tz":-540,"elapsed":1209,"user":{"displayName":"Shuntaro Shimizu","photoUrl":"","userId":"02252545979345483385"}}},"source":["for i in range(10):\n","    plt.subplot(2, 5, i + 1)\n","    plt.imshow(X[i * 6500].reshape(28, 28), cmap='gray_r')\n","    plt.axis(\"off\")"],"execution_count":5,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAXIAAADSCAYAAABXT0tTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAGX1JREFUeJzt3WeUFEUXxvE/ZkXFnDMi6DGimEXE\nAKJHMSMqiqIoioo541HMmHMOcEwoZjEhRsyKxwSKEQUVA+as7wfeZ2qmd2Z3did1Dc/vyw6zs9O9\nTc/d29W3brX677//MDOzeM1U6x0wM7PSOJCbmUXOgdzMLHIO5GZmkXMgNzOLnAO5mVnkHMjNzCLn\nQG5mFrlZqry9GWX2UatmvNbHpCEfk/x8XBryMcEZuZlZ9BzIzcwi50BuZhY5B3Izs8g5kJuZRa7a\nVStmNbXbbrsBMGLECADmm28+AHr37g3AOuusA8COO+4IwLzzzlvtXTRrNmfkZmaRa1XlhSVqVvP5\n2muvAXDZZZcBcPPNNwOw9957AzBw4EAAOnbsWI7NpbIOduLEiQCceeaZOc+feOKJALRt27aSm0/F\nMbnwwgsBuPXWW4FwXrRqlbt7q6yyChDOiwMOOKASu+M68vxSca6kjOvIzczqWd1n5OPGjQNgs802\nA+DHH3/M+7o2bdoA8N1335Vjs6nMKEaNGgVAjx49cp5/5JFHAOjWrVslN5+qY/Lbb78B8PPPPwNw\n33335XwdPXo0AH/88QcA/fv3B+CKK64o525UJSPv06dP5vGwYcMafe3yyy8PQN++fXOe172FlVZa\nqSW70FypOlcKufbaa4FwbiyzzDIAPP744wC0a9eunJtzRm5mVs/qNiN/+eWXAdhpp50A+OKLL4Aw\nFqpqhNlmmw2Ab775BoDnnnsOgLXXXjvzXnpNM6QyoyiUkQ8ePBiAU089tZKbT+UxKeTFF18EYNCg\nQQCMHz8egFNOOSXn+RKVJSMfO3YsAGeffTYAjz32WM73//zzz+bvWcIss0wvcDvkkEMAuOCCC0p+\nz0ZEca7su+++QLjfJltvvTUADz74YDk354zczKye1U1G/uuvvwLw+uuvA7DnnnsCMGnSpOkb/v/v\nqYxcGfcxxxwDhDFAvW7IkCGZ9z7hhBOauzupzCgKZeSdOnUCwlVMhaTymDTlmWeeAaBLly5AuJfy\n0UcfATD//POX8vZlycjnmGMOoDyZd1Pat28PwHvvvVfJzURxrjgjNzOzsnEgNzOLXN1M0VcJkCZ6\nNEUTQVR+tummmwLw1FNPAfDWW2+VeQ8tRp07dwZgwIABQCg/nDp1KlDy0EpZrLHGGgC88sorzf7Z\n5ZZbDoDu3bvnPP/AAw8AoUjAitezZ8+qb9MZuZlZ5KLPyJVZ68ZC8uatblJtu+22ABx11FEALLHE\nEgCstdZaQMisxowZk/d9zCDcLL/99tuBUI5YS0888QQQbmZrQoroHAfYYYcdcr43++yzA7DAAgsA\nMHnyZCB8DmZkL730EhBuIm+yySZF/VyHDh0qtk+FOCM3M4tctBm5pt5vscUWQJh6r4xJJXa33XYb\nEMa+zzjjDAD69esHwMILLwyEcUb9/EMPPZTZlkoay9RQK3UOPfTQWu9Camkqf3L8+e23367F7uQ1\nzzzzALDrrrvmfC2GrmSnTZsGhKZyEyZMyPv6n376CYCPP/4YCFP668mll14KhNJkZeQ33XQTAHvt\ntVejP6/S1I033rhCe9iQM3Izs8hFl5G///77AJx77rkA/PDDD0DIrBdffHEgtKede+65gTBGrq9N\n0QQjgKFDhwLFV8Sk1eWXX573eR07a+jGG28EQkauK7YjjjiiZvtUCrUwVvOsTz75BAjNwZqiMfSN\nNtoICNViavMQoylTpgChCdZZZ50FNJxgpVjSlFpUvDkjNzOLXBQZeXa2oKoTjWGr+dUtt9wChKW6\nNLZZDprmH7uvvvoq599a5myhhRaqxe5EodBYeIUX4Sire+65J/P4tNNOA0qfzv/ll18CIcN/8skn\nAXj66adLet9qyW4spquJplpUJCt+Ctl+++1bvmMt5IzczCxyUWTkqhqB3GoSCAsBaGamFW/VVVcF\nclv2zqg0VqyZvmqEdNVVVwGh3lozO2O6r5C9WEqxmbiu1tZff30gHBe1eZa//voLCG1/VWutK2RI\nV2WLZuQeeeSRmefefffdvK9VJZtiTLE0arDHHnsAocpFx7QSnJGbmUUuiow8u0JAMy41Y7PcmXi+\nGZ2e5Vk/XnjhBQAeffRRIIzpvvnmm0CoghJVqWhhBd2TqTcLLrggEMaBtej0aqutBsAvv/wChB4s\nvXv3zvl5ZebPP/88AOedd17me2VeHq9FdFWi+SWFsnCA1VdfHQgzZm+44QYgXM2ooq1QFZsqnPRV\nbbA//PDDzGsWWWSRFvwWhTkjNzOLXKozcs060yxOCBnSdtttV5Ft6v31FWDNNdesyLascv7++28g\nZEW77747ECp3Zp555pyvM83UeE6jjFQLluiKUJlsmmUvJKJFpZP0eygbTWrdujUQPnf77bcfANdf\nf33e11999dWZxxtuuCEQjl01ffvttwCcfPLJQOjN1JgNNtgACJ0hdX8guThNIbp6U48bbbvcWXg2\nZ+RmZpFLdUauWvDsO+36q6al2UqlGvXkwsObb7555rEWtbV0y+6dfdFFFwFhkWAtoK3/56222goI\nGfnpp58OwMiRI4FQYaD/e1VlqN+GOg3WIstsLs12Tj5uibnmmguAiy++GIB///0XCDNgJfu+kiqC\nakF187pCaCqbzn6t6GcWW2wxALp16waEfjTqkig6t8q81FujnJGbmUUu1Rl5PlpottTMQpm4FllW\n75all14ayK0zLbbHgtWGKlBUvwvwzjvvANCuXTsA7rzzTiDUBouqNO6//34A5pxzTgBGjBgBQNeu\nXYEwbqqMXB3uZlTKzJXxJjPybKruOOmkkyq/YwnNWcFpmWWWAcKcAc0SV4fVffbZJ+f1unrbZZdd\nSt3NkjkjNzOLXHQZeanVKqqAUQZ+xx13AKE/gv7Kzgh+//13INTFKsuKhXpjq+Z54sSJme8tuuii\nQOgBsuSSSwKhb456sN97771AGBNXv3pl4vLss88CYey32NViDMaPH1+zbR933HFAmMXcGGXexc7A\nVI19GjgjNzOLXKozcmU/2XfAlUHprnmxVL2g6gTN4FPVQXZviHqllWTk1VdfBUK2qbvxsVD2rBlz\n2auXH3/88UDIxHXlddhhhwGh54ayL42JKytL0pj7/vvvD1R39Zdq+P7774HmjSln/1xa6Z7azjvv\nXOM9qSxn5GZmkUt1Rp5vlqX6IGuMc9999wXCzDTV+2oFFPXQ0NjosssuC0D37t0BGDBgQOV+gZQ5\n+uijgfpZIV1ZtmTPslRPlD59+gBh7VbVPas6RZ3tOnfu3Oi21MlOlQyzzjprSfueFuo1c/jhhwNh\nXdq+ffsCTXcuVD/yxtRrf5pCVFf+wQcfAKFyqpKckZuZRS7VGXk+6qGh9SfvuusuANq0aQOENT2T\n1O9B1QiqfzV4+OGHgfjGyNVV7sADDwRClzoIPUB0NaeZnar5HTRoEBD6YTRFdeT1QjNc9TlSTxJd\nwTZWF95c2fX9MwJ1WlR/nmpwRm5mFjkHcjOzyKV6aEWXs+uuu27mueQCqbr5mVxYWAsK9+rVC2h+\nuWI9at++PRDac6qZkRZbiI1uyKnNaPaNN93U1AII/fr1AxpO0Z9RqSHdtGnTKvL+2dPW1UTKKscZ\nuZlZ5FpVeRmzFm1sypQpmcdqMamJPclm75rwcdBBBwHVKf3Jo+lemUHV15EbPHgwEG74durUCWh4\ntVNmqT4mNdKcYwIVOC6awp68YlXr6GLjgyZW6dzS57CFojhXVF7YoUOHvN/XIhZlWpim0WPijNzM\nLHJRZOQRiiKjqDIfk4ZqnpEXMnz4cKDpRSHU9qHEDDwpinNFJZtqB6GS1+TCElq8pETOyM3M6pkz\n8sqIIqOoMh+ThlKbkdeYz5WGnJGbmdUzB3Izs8g5kJuZRc6B3Mwscg7kZmaRq3bVipmZlZkzcjOz\nyDmQm5lFzoHczCxyDuRmZpFzIDczi5wDuZlZ5BzIzcwi50BuZhY5B3Izs8g5kJuZRc6B3Mwscg7k\nZmaRcyA3M4ucA7mZWeQcyM3MIudAbmYWOQdyM7PIOZCbmUXOgdzMLHIO5GZmkXMgNzOLnAO5mVnk\nHMjNzCLnQG5mFjkHcjOzyDmQm5lFzoHczCxyDuRmZpFzIDczi5wDuZlZ5BzIzcwi50BuZhY5B3Iz\ns8g5kJuZRc6B3Mwscg7kZmaRcyA3M4ucA7mZWeQcyM3MIudAbmYWOQdyM7PIOZCbmUXOgdzMLHIO\n5GZmkXMgNzOLnAO5mVnkHMjNzCLnQG5mFjkHcjOzyDmQm5lFzoHczCxyDuRmZpFzIDczi5wDuZlZ\n5BzIzcwi50BuZhY5B3Izs8g5kJuZRc6B3Mwscg7kZmaRm6XK2/uvyturlVbNeK2PSUM+Jvn5uDTk\nY4IzcjOz6DmQm5lFzoHczCxyDuRmZpFzIDczi1y1q1YsxX755RcAevfuDcB+++0HwHbbbVezfTKz\npjkjNzOLnDNyy7jnnnsAuP/++wF49913AejatSsAc889d212zMwa5YzczCxyM0xGrmxzwoQJAOy/\n//4ALLjggjmv++STTwB4//33M89NmTIFgCeffBKANm3aAHDJJZdUboeraPjw4QCceOKJOc/PP//8\nAMw666xV3yeL25AhQzKPf/31VwD69+8PwLLLLluTfapnzsjNzCLX6r//qtqqoGob+/777wEYOHAg\nAHfccQcA//zzDxCyza233hqA0aNHAyF7+OmnnzLvpWPUqtX0dgczzzwzAJ06dQJg7Nixyc2nuleE\nqlN69eoFwIMPPpjz/cUWWwyAYcOGAbDFFluUY7OpPiY1kppeK/q8/PzzzwB88MEHAKyyyipAOCea\ncuqppwJwzjnnZJ77/fffgfCZO/vsswE44IADCr2Nz5WG3GvFzKye1V1G/thjjwHhr/2nn34KhGy6\nwQ4lsu2WvObff/9NPpXqjGL77bcHQnWKzDLL9FsmV111FRDqyMuk5GOiTO6EE07Ief7www8v+Ea6\nF6KKm9lnnx2ARRZZpBm7UzE1y8h/++03AIYOHQrAtddeC8CkSZNyXjfvvPMC0K9fPwDOP//8Rt93\n8cUXB+DLL78s+Jq+ffsCcMMNNxR6Sao/P6rm+vDDDwF48803gfB5euWVV3Jer3Mv+8p30003be5m\nnZGbmdUzB3Izs8jVzdDKNddcA8AxxxwDwI8//jh9g00Mi5QytNK2bVsg3BjKkspLw88//xyAdu3a\nAeEmlFx88cUAHHrooZXYfMnHZOTIkUAok1SJaHP+DxdaaCEANttss5zvb7LJJgCsvfbaOc9rqGC5\n5ZYrfu+LV7OhlULDa4Xo2B599NFA7s1MCDdLV1xxRQC+++67gu+lY/3MM88U3FxROzVd1T4/Rxxx\nBABXXHEFAH/++Wfe1+mm7sYbbwzAqFGjAJhnnnkyr7n77rsB6NKlS7Gb99CKmVk9iz4jX2GFFQD4\n+OOPm/Vz66yzDgDbbrstALvssgsQyq0a8/bbbwNhMpGytiypyihUUrb66qsD4VjNNNP0v+PKxA8+\n+GCg8cy2BGU7Jt9++y0QSkqV2el5CJO3Mm/Ywisz/d/utddeQDhGSy21VFO/QzFqlpGfdtppAAwe\nPDjv95VVzjHHHEAoy9WNfd24W3PNNYHwOXr22WcLblM3+HS+rbHGGoVemorPjz43q622GhBu4Kr8\nWFl1+/btc35ON9R17ugK9/LLL8+8pgWfOWfkZmb1LIqMfPLkyZnHKo1TuZT+Sib/oimjUKnazjvv\nDIQMQ9+fbbbZWrJLTUlFRvHXX38BYUwvWaZ37rnnAmHcs8Iqfkz++OOPzOOpU6fmfO+pp54CQqlY\nkjLNhx9+GMht0QAhY7/zzjuBcD6VqGYZudpOLLHEEjnP6/PxwgsvACHjVgapc+iUU04B4Omnn875\nms/6668PwNVXXw2EK8NG1PTzoytWlVyOGTMGgG7dugFw+umnA+GqvinXXXcdAGeeeWbmObUCeeSR\nRwDYaqutmnobZ+RmZvUsioxclSgQJjBk3jAxtqm/mqpiWXrppVuyyVKlIiMfMWIEALvuumvO87pT\nfu+99wKhCViFpeKYFEsZ+XrrrQfAtGnTgDDRTA3TNB7aQjXLyPW5ee+994CQEX7xxRdAGPe98sor\nAZhrrrmAkF2rkZqu+pJ22GGHzGNN5FpppZWK3b2anCtPPPEEED4vGiO/6KKLANhjjz2Awp+Xk046\nCQjVKaoO089lXzGKmvmpiqgRzsjNzOpZ3bSx1Zi3liXTdPMZ0euvvw5Anz59cp7X2KQqDlq3bl3d\nHYuIKmD+/vtvIFzxqUJG2VqJGXnN6PdRlZamlav6ROeQKi6SWWihTHz55ZcH4MILL8w8l/a2tZpP\ncfLJJwPh//bll18Gwn0CUf24joHGzM877zwgXO3oqkXVYdnVLbpi0fEulTNyM7PIRZG2ZrfQLDSm\nr5lkAwYMyPmq2VU77bQTEBpBZc+yqjca+1amoUqEyy67DHAmXgzNglX9tGjuQHJBktip5lnVKp07\ndwbgpZdeKurnd9xxRwBuvfVWIK4rFVV16XdVZY4aq6m5l1pb33LLLQC88cYbed9PTbL0Ot0bKGaO\nSks5Izczi1wUGblm1QHceOONALzzzjtA0zP2nnvuOQCef/55AC644AIgVHSoKqEeqN+F6nXloIMO\nAkKPC9VKb7TRRkDVqlbqQrIXS71RrbPGyAuZc845gVBPf/311wNxLguY7JWkKpW77roLCGPhX331\nVaPvoxmgmi3bs2fPsu5nY5yRm5lFLoqMfOGFF848Vg8NzUxTpq2ZUg888AAQZmclO/xp7LNHjx5A\nmJG26qqrVmLXq0pXK19//TUQOv1pEYEFFlgACJm7Fg1Q74h1110XCDM+i5iBV7dUnaIrPn3VVU3s\nVNOsOnItwKE+QoWqUtTbSOeI7j3FTH1ndJWvY6DqFS11uPLKKwPwww8/AGEugWaHa+RA9wuqyRm5\nmVnkopjZ2RIa41OPCI2JJzP0LbfcEgiVHhr7K1FNZqadddZZQMOl0JpLVULqZKce0yWKYmanZnSq\n2kn15Mq6dH8h2c+8hao+s1MVGKrzVvZZLM3TuO+++0rdlcak8lx58cUXgXDPSX17dHVy1FFHVXLz\nntlpZlbPohgjV/0mhH4G6n2R7CcuHTt2BODmm28GQoWGOv1pBSEt1qxa0iOPPLL8v0DKaMxPs191\nN151seooqQofHZsZgeqos3ubQ8jQy5SJV4U6Nd5+++2Z53TlWehKXDOkdR/FAi3knuyg2aFDh1rs\nTg5n5GZmkUt1Rv7ZZ58B4Y46wOOPP57zGmWTyYw8SV3r1M1Nf1WVmSjDryf6nVWZs88++wAwaNAg\nIMxa3HvvvYGGM9XS3iOjEjQGnqQrwRiojlljt8n7Qvno/1pzELp37573darcmJEk+/pr1uqjjz4K\nwIYbblibHcvijNzMLHKpzsjVdayxbLlc/QvUEVCdzGKU7Pe8wQYbAKFCQT1Xxo4dC4TOdslMXP2Y\njz322MrtbMqoy6GuUpJjyFpvMgaqi26MMu6+ffsC4b5JoZmr22yzDQCHHHJIOXYxCv/88w8Q1kNQ\nFdfAgQOBcBWThk6rzsjNzCJX+z8ljdAd9MYycq3qobpxVavIsGHDgLDitWZ8JikjiZkqKnTcVLWg\n4zdy5Eig8ErnnTp1AmasKhXRTGGNkat3T3JNyxittdZamcfqI6JxXVXp6Mq2UD8RrXaz1FJLVWw/\n02b48OFAmIuiXutDhgwB0tVB1Rm5mVnkUp2RqxLlnHPOKfiat956C2h5VzqtVVjMuGLaqZdK//79\ngbBW4qhRo/K+Xr1WVMWiGvo0ZRrVUujeSK9evaq8J6XTePZDDz0EhHU4IfRW0SxgzVYsdNWr+RfZ\nHUjrnXoTHXfccTnPd+3aFUjn58MZuZlZ5BzIzcwil+qmWWpNu/vuu2eemzRpEgCTJ0+e/oZNLCyR\n/L6aH6mESEMqZWqWJTVt+qM2tloMdujQoUAol1KbzYMPPhiAtm3blnsX8kllIyTRArnJ80itTSs0\nDbsiTbN0A1PLHY4bN66Zmwmlv1quTIsmVElNzhVNQNxtt92AsPSbSi410apGy/y5aZaZWT1LdUae\nz9SpU4EwqUU3QidOnAjAN998k7vB//9+iy66KADXXXcdEJptVUiqs88aSeUxUUM23fhVRq5JUdkN\npyqgom1sVVJ52GGHZZ5TO+ckLd6iEtabbroJKPuVarGqeq5o4o9ueOsqXVclWrS8xlPxnZGbmdWz\n6DLyQpRZqZwqSVOsNUZeYanMPmsslcdk8803B2DMmDFAyMhV+hpzRi7Zk3w0GUwTn7Swij4fKZkA\nVdVzRa19de+odevWQFh8uVu3bqVuohyckZuZ1bO6ychTJpXZZ42l6piMHj0aCPdKtBixMnK1dOjZ\ns2cld6PqS71FoqrnSpcuXYCw4LaqU/Q1JZyRm5nVs1RP0TerFLX41Rix2hhr4ZEePXrUZses6rSg\ntOYMxPh/74zczCxyHiOvjFSNB6eEj0lDHiPPz+dKQx4jNzOrZ9XOyM3MrMyckZuZRc6B3Mwscg7k\nZmaRcyA3M4ucA7mZWeQcyM3MIudAbmYWOQdyM7PIOZCbmUXOgdzMLHIO5GZmkXMgNzOLnAO5mVnk\nHMjNzCLnQG5mFjkHcjOzyDmQm5lFzoHczCxyDuRmZpFzIDczi5wDuZlZ5BzIzcwi50BuZha5/wFB\n6aNBXc/CiwAAAABJRU5ErkJggg==\n","text/plain":["<Figure size 432x288 with 10 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"W-zZtlXz0sVW","colab_type":"text"},"source":["### 学習用・テスト用データの分割\n","- 訓練データで学習し，同じ訓練データで性能の評価を行うと，訓練データでは良い性能を示すが，データを少しでも変えると性能が低下してしまうことがある（**過学習**）．\n","- よって，学習する訓練データとは異なるテストデータで性能評価を行う．"]},{"cell_type":"code","metadata":{"id":"yNEB4-H60sVX","colab_type":"code","colab":{}},"source":["train_x, test_x, train_y, test_y = train_test_split(X, Y, test_size=0.2, random_state=2)\n","train_y = np.eye(10)[train_y].astype(np.int32)\n","test_y = np.eye(10)[test_y].astype(np.int32)\n","train_n = train_x.shape[0]\n","test_n = test_x.shape[0]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lkr-Eal00sVb","colab_type":"text"},"source":["# 4. softmax関数の実装\n","活性化関数の一種であるsoftmax関数を実装する．\n","- 関数：softmax\n","    - 入力：$\\boldsymbol{X}=(\\boldsymbol{x_1},\\boldsymbol{x_2},\\cdots,\\boldsymbol{x_N})^\\top\\in\\mathbb{R}^{N\\times K}$\n","\n","    - 出力：$\\boldsymbol{Y}=(\\boldsymbol{y_1},\\boldsymbol{y_2},\\cdots,\\boldsymbol{y_N})^\\top\\in\\mathbb{R}^{N\\times K},\\,\\,\\,y_{nk} = softmax(\\boldsymbol{x_n})_k$\n","    - オーバーフローを防ぐために$\\boldsymbol{x}_n$の最大値を$\\boldsymbol{x}_n$自身から引く\n","$$\n","\\begin{align}\n","softmax(\\boldsymbol{x})_k&= \\frac{\\exp (x_{k})} {\\Sigma_{i=1}^{K}{\\exp (x_{i})}}\\\\\n","&=\\frac{\\exp (-x_{max})\\exp (x_{k})}{\\exp (-x_{max})\\Sigma_{i=1}^{K}{\\exp (x_{i})}}=\\frac{\\exp (x_{k}-x_{max})} {\\Sigma_{i=1}^{K}{\\exp (x_{i}-x_{max})}}\n","\\end{align}\n","$$"]},{"cell_type":"markdown","metadata":{"id":"1zrpt_q00sVc","colab_type":"text"},"source":["<details>\n","<summary>\n","ヒント\n","</summary>\n","<ol>\n","    <li>最大値\n","    <ul> \n","        <li>```axis=1```を指定するとデータ$\\boldsymbol{x_n}$ごとに最大値を計算できる</li>\n","        <li>行列のshapeを変えたくない場合は，```keepdims=True```を指定する</li>\n","    </ul></li>\n","    <li>$\\exp$\n","    <ul>\n","    <li>```np.exp()```</li>\n","    </ul></li>\n","    <li>合計\n","    <ul>\n","    <li>```np.sum()```</li>\n","    <li>```axis=1```を指定するとデータ$\\boldsymbol{x_n}$ごとに合計を計算できる</li>\n","    <li>行列のshapeを変えたくない場合は，```keepdims=True```を指定する</li>\n","    </ul></li>\n","</ol>\n","</details>"]},{"cell_type":"code","metadata":{"id":"FbcZVE610sVc","colab_type":"code","colab":{}},"source":["def softmax(x):\n","    x_max = np.max(x, axis=1).reshape(-1, 1)\n","    exp_x = np.exp(x - x_max)\n","    y = exp_x / np.sum(exp_x, axis=1).reshape(-1, 1)\n","    return y"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"q-vUNsC50sVg","colab_type":"text"},"source":["テスト．以下のセルを実行"]},{"cell_type":"code","metadata":{"id":"aZ5nQ-Se0sVg","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"2bcdd41b-aba7-4dfe-ee40-7f5bc9c9964e","executionInfo":{"status":"ok","timestamp":1557817061071,"user_tz":-540,"elapsed":376,"user":{"displayName":"Shuntaro Shimizu","photoUrl":"","userId":"02252545979345483385"}}},"source":["test_softmax(softmax)"],"execution_count":10,"outputs":[{"output_type":"stream","text":["ok!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"K2NRmZt70sVj","colab_type":"text"},"source":["# 5. 多クラスの交差エントロピー誤差の実装\n","- 関数：cross_entropy\n","    - 入力： $Y=(\\boldsymbol{y}_1,\\boldsymbol{y}_2,\\cdots,\\boldsymbol{y}_N)^\\top\\in \\mathbb{R}^{N\\times K}$, \n","$T=(\\boldsymbol{t}_1,\\boldsymbol{t}_2,\\cdots,\\boldsymbol{t}_N)^\\top\\in \\mathbb{R}^{N\\times K}$<br />\n","$\\boldsymbol{y}_n$はソフトマックス関数の出力，$\\boldsymbol{t}_n$は教師ラベル(1-of-K表現)\n","\n","    - 出力： \n","    $$L=-\\frac{1}{N}\\sum_{n=1}^N \\sum_i \\boldsymbol t_{n,i} \\log \\boldsymbol y_{n,i}\\in\\mathbb{R}^1\n","$$\n"]},{"cell_type":"markdown","metadata":{"id":"ERm53VeX0sVk","colab_type":"text"},"source":["<details>\n","<summary>\n","ヒント\n","</summary>\n","<ol>\n","    <li>$\\log$\n","    <ul> \n","        <li>```np.log()```</li>\n","    </ul></li>\n","    <li>合計\n","    <ul>\n","    <li>```np.sum()```</li>\n","    </ul></li>\n","    <li>平均\n","    <ul>\n","    <li>```np.mean()```</li>\n","    </ul></li>\n","</ol>\n","</details>"]},{"cell_type":"code","metadata":{"id":"ObGdYiNQ0sVl","colab_type":"code","colab":{}},"source":["def cross_entropy(y, t):\n","    L = -1.0 * np.sum(t * np.log(y)) / len(y)\n","    return L"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EWvg50d10sVo","colab_type":"text"},"source":["テスト．以下のセルを実行"]},{"cell_type":"code","metadata":{"id":"HXiQGpZL0sVp","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"142976ff-c55b-452b-8071-b96018832902","executionInfo":{"status":"ok","timestamp":1557817207016,"user_tz":-540,"elapsed":1337,"user":{"displayName":"Shuntaro Shimizu","photoUrl":"","userId":"02252545979345483385"}}},"source":["test_cross_entropy(cross_entropy)"],"execution_count":12,"outputs":[{"output_type":"stream","text":["ok!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"fsq6sAK40sVr","colab_type":"text"},"source":["# 6. ソフトマックス回帰の実装\n","ソフトマックス回帰クラスを実装する．\n","## 6.1 勾配降下法の実装\n","SoftmaxRegressionクラスのgradient_decent関数を実装してください．\n","- 関数：gradient_descent  \n","    - 入力：\n","        - 学習データ： $\\boldsymbol{X}\\in\\mathbb{R}^{N\\times D}$\n","        - 予測ラベル：$\\boldsymbol{Y}\\in\\mathbb{R}^{N\\times K}$\n","        - 教師ラベル：$\\boldsymbol{T}\\in\\mathbb{R}^{N\\times K}$\n","        - 学習率：$\\epsilon \\in \\mathbb{R}$\n","    - 更新：\n","        - 重みとバイアス $\\boldsymbol{W},\\,\\boldsymbol{b}$\n","        \n","- 勾配降下法: （$\\boldsymbol{W},\\boldsymbol{b}$：パラメータ，$\\epsilon$：学習率）\n","$$\n","\\boldsymbol{W}\\leftarrow\\boldsymbol{W}-\\epsilon\\nabla_{\\boldsymbol{W}}L\\\\\n","\\boldsymbol{b}\\leftarrow\\boldsymbol{b}-\\epsilon\\nabla_{\\boldsymbol{b}}L\n","$$\n","- ソフトマックス回帰の勾配 :\n","\\begin{align}\n","\\nabla_{\\boldsymbol{W}}L&=\\frac{1}{N}\\boldsymbol{X}^\\top(\\boldsymbol{Y}-\\boldsymbol{T})\\\\\n","\\nabla_{\\boldsymbol{b}}L&=\\frac{1}{N}(1,1,...,1)(\\boldsymbol{Y}-\\boldsymbol{T})\n","\\end{align}"]},{"cell_type":"markdown","metadata":{"id":"aPfu71cu0sVs","colab_type":"text"},"source":["<details>\n","<summary>\n","ヒント\n","</summary>\n","<ol>\n","    <li>行列の積\n","    <ul> \n","        <li>```np.dot()```</li>\n","    </ul></li>\n","    <li>合計\n","    <ul>\n","    <li>```np.sum()```</li>\n","    </ul></li>\n","</ol>\n","</details>"]},{"cell_type":"code","metadata":{"id":"zWPjvYq90sVt","colab_type":"code","colab":{}},"source":["class SoftmaxRegression:\n","    def __init__(self, n_in, n_out):\n","        self.W = np.random.uniform(0.08, -0.08, (n_in, n_out)) #勾配の初期化\n","        self.b = np.zeros(n_out) #バイアスの初期化\n","        \n","    def gradient_decent(self, X, Y, T, eps):\n","        batchsize = X.shape[0]\n","        self.W -= eps * np.dot(X.T, (Y - T)) / batchsize\n","        self.b -= eps * np.dot(np.ones(batchsize), (Y - T)) / batchsize\n","        \n","    def train(self, x, t, lr):\n","        y = softmax(np.dot(x, self.W) + self.b) #予測\n","        self.gradient_decent(x, y, t, lr) #パラメータの更新\n","        loss = cross_entropy(y, t) #ロスの算出\n","        return y, loss\n","\n","    def test(self, x, t):\n","        y = softmax(np.dot(x, self.W) + self.b) #予測\n","        loss = cross_entropy(y, t) #ロスの算出\n","        return y, loss"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N_rNHRNX0sVv","colab_type":"text"},"source":["テスト．以下のセルを実行"]},{"cell_type":"code","metadata":{"id":"kdwl9LAo0sVw","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"0f781ac6-41da-407d-9001-d69aff0ac3e0","executionInfo":{"status":"ok","timestamp":1557817496521,"user_tz":-540,"elapsed":545,"user":{"displayName":"Shuntaro Shimizu","photoUrl":"","userId":"02252545979345483385"}}},"source":["test_gradient_decent(SoftmaxRegression)"],"execution_count":15,"outputs":[{"output_type":"stream","text":["ok!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Mcwr-bmQ0sV0","colab_type":"text"},"source":["# 7. 学習\n","## 7.1 モデルの初期化\n","入力は784次元，出力は10次元"]},{"cell_type":"code","metadata":{"id":"q1c4NKEb0sV0","colab_type":"code","colab":{}},"source":["model = SoftmaxRegression(784, 10)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yKwwvm4k0sV3","colab_type":"text"},"source":["## 7.2 ハイパーパラメータの設定\n","- 学習epoch数は20\n","    - epoch数とは，学習データを何回学習するかを表す数である．\n","- バッチサイズは100\n","    - ミニバッチとは少数のサンプルからなる集合である．\n","- 学習率は1"]},{"cell_type":"code","metadata":{"id":"LUY-Z1s60sV4","colab_type":"code","colab":{}},"source":["n_epoch = 20\n","batchsize = 100\n","lr = 1"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"i7g0yf9n0sV6","colab_type":"text"},"source":["## 7.3 学習\n","交差エントロピー誤差を確率的勾配降下法を用いて最小化する．"]},{"cell_type":"code","metadata":{"id":"3hoW6nbq0sV7","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":357},"outputId":"6c04f8d0-87a2-48a0-a7f1-af1af4aff9bd","executionInfo":{"status":"ok","timestamp":1557817517280,"user_tz":-540,"elapsed":13551,"user":{"displayName":"Shuntaro Shimizu","photoUrl":"","userId":"02252545979345483385"}}},"source":["for epoch in range(n_epoch):\n","    print ('epoch %d |　' % epoch, end=\"\")\n","    \n","    # Training\n","    sum_loss = 0\n","    pred_label = []\n","    perm = np.random.permutation(train_n) #ランダムに並び替える\n","    \n","    for i in range(0, train_n, batchsize): #ミニバッチごとに学習を行う\n","        x = train_x[perm[i:i+batchsize]]\n","        y = train_y[perm[i:i+batchsize]]\n","        \n","        pred, loss = model.train(x, y, lr)\n","        sum_loss += loss * x.shape[0]\n","        # pred には， (N, 10)の形で，画像が0~9の各数字のどれに分類されるかの事後確率が入っている\n","        # そこで，最も大きい値をもつインデックスを取得することで，識別結果を得ることができる\n","        pred_label.extend(pred.argmax(axis=1))\n","\n","    loss = sum_loss / train_n\n","    # 正解率\n","    accu = accuracy_score(pred_label, np.argmax(train_y[perm], axis=1))\n","    print('Train loss %.3f, accuracy %.4f |　' %(loss, accu), end=\"\")\n","    \n","    \n","    # Testing\n","    sum_loss = 0\n","    pred_label = []\n","    \n","    for i in range(0, test_n, batchsize):\n","        x = test_x[i: i+batchsize]\n","        y = test_y[i: i+batchsize]\n","        \n","        pred, loss = model.test(x, y)\n","        sum_loss += loss * x.shape[0]\n","        pred_label.extend(pred.argmax(axis=1))\n","        \n","    loss = sum_loss / test_n\n","    \n","    accu = accuracy_score(pred_label, np.argmax(test_y, axis=1))\n","    print('Test loss %.3f, accuracy %.4f' %(loss, accu) )"],"execution_count":18,"outputs":[{"output_type":"stream","text":["epoch 0 |　Train loss 0.419, accuracy 0.8810 |　Test loss 0.306, accuracy 0.9145\n","epoch 1 |　Train loss 0.314, accuracy 0.9104 |　Test loss 0.297, accuracy 0.9166\n","epoch 2 |　Train loss 0.305, accuracy 0.9131 |　Test loss 0.302, accuracy 0.9156\n","epoch 3 |　Train loss 0.293, accuracy 0.9175 |　Test loss 0.312, accuracy 0.9108\n","epoch 4 |　Train loss 0.288, accuracy 0.9175 |　Test loss 0.294, accuracy 0.9182\n","epoch 5 |　Train loss 0.285, accuracy 0.9194 |　Test loss 0.286, accuracy 0.9202\n","epoch 6 |　Train loss 0.284, accuracy 0.9194 |　Test loss 0.296, accuracy 0.9173\n","epoch 7 |　Train loss 0.281, accuracy 0.9202 |　Test loss 0.290, accuracy 0.9185\n","epoch 8 |　Train loss 0.277, accuracy 0.9215 |　Test loss 0.299, accuracy 0.9201\n","epoch 9 |　Train loss 0.279, accuracy 0.9203 |　Test loss 0.328, accuracy 0.9074\n","epoch 10 |　Train loss 0.275, accuracy 0.9227 |　Test loss 0.330, accuracy 0.9081\n","epoch 11 |　Train loss 0.272, accuracy 0.9231 |　Test loss 0.292, accuracy 0.9212\n","epoch 12 |　Train loss 0.270, accuracy 0.9244 |　Test loss 0.316, accuracy 0.9099\n","epoch 13 |　Train loss 0.273, accuracy 0.9230 |　Test loss 0.297, accuracy 0.9182\n","epoch 14 |　Train loss 0.271, accuracy 0.9230 |　Test loss 0.302, accuracy 0.9159\n","epoch 15 |　Train loss 0.269, accuracy 0.9235 |　Test loss 0.303, accuracy 0.9181\n","epoch 16 |　Train loss 0.269, accuracy 0.9245 |　Test loss 0.317, accuracy 0.9150\n","epoch 17 |　Train loss 0.267, accuracy 0.9253 |　Test loss 0.302, accuracy 0.9164\n","epoch 18 |　Train loss 0.267, accuracy 0.9247 |　Test loss 0.291, accuracy 0.9217\n","epoch 19 |　Train loss 0.266, accuracy 0.9243 |　Test loss 0.310, accuracy 0.9146\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"wUSJ15NA0sWB","colab_type":"text"},"source":["テストの正解率が92%程度になると成功です．"]}]}